{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use autokeras to find a model for the sonar dataset\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autokeras import StructuredDataClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = '/home/danish/Datasets/ICS/multiclass'\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.arff')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and concatenate its data to the combined DataFrame\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df , _ = arff.loadarff(file_path)\n",
    "    df = pd.DataFrame(df)\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Display the combined DataFrame\n",
    "data = combined_df # .sample(frac=0.5, random_state=42)\n",
    "X = data.drop('marker', axis=1)\n",
    "y = data['marker'] \n",
    "\n",
    "# Optionally, you can save the combined DataFrame to a new CSV file\n",
    "# combined_df.to_csv('/path/to/combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78377, 128) (78377,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # split into input and output elements\n",
    "# data = dataframe.values\n",
    "# X, y = data[:, :-1], data[:, -1]\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic data preparation\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52512, 128) (25865, 128) (52512,) (25865,)\n"
     ]
    }
   ],
   "source": [
    "# separate into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./structured_data_classifier/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./structured_data_classifier/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# define the search\n",
    "search = StructuredDataClassifier(max_trials=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/2\n",
      "1641/1641 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0205\n",
      "Epoch 2/2\n",
      "1641/1641 [==============================] - 4s 3ms/step - loss: nan - accuracy: 0.0205\n",
      "INFO:tensorflow:Assets written to: ./structured_data_classifier/best_model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff90ed94588>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform the search\n",
    "search.fit(x=X_train, y=y_train, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.022\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, acc = search.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best performing model\n",
    "model = search.export_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "multi_category_encoding (Mul (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "normalization (Normalization (None, 128)               257       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 37)                37925     \n",
      "_________________________________________________________________\n",
      "classification_head_1 (Softm (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 76,102\n",
      "Trainable params: 75,845\n",
      "Non-trainable params: 257\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer MultiCategoryEncoding with weights [<tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677f9d68>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fc358>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fc908>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fceb8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fa4a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677faa90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677800b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677806a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867780c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677832b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867783898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867783e80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677874a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867787a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677880b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677886a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867788c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677942b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867794898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867794e80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677994a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867799a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677a50b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677a56a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677a5c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677ad2b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677ad898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677ade80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677b24a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677b2a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677bb0b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677bb6a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677bbc88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677452b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867745898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867745e80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86774b4a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86774ba90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677500b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677506a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867750c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86775a2b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86775a898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86775ae80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677614a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867761a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677680b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677686a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867768c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677712b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867771898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867771e80>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-15e17a72240b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save the best performing model to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/danish/Codes/Auto_Keras_Dataset_3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Auto-Keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 2107\u001b[0;31m                     signatures, options, save_traces)\n\u001b[0m\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/anaconda3/envs/Auto-Keras/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    145\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    146\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 147\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedObjectSavingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Auto-Keras/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Auto-Keras/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Auto-Keras/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36m_legacy_weights\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;34m'Save or restore weights that is not an instance of `tf.Variable` is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;34m'not supported in h5, use `save_format=\\'tf\\'` instead. Got a model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         'or layer {} with weights {}'.format(layer.__class__.__name__, weights))\n\u001b[0m\u001b[1;32m    899\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer MultiCategoryEncoding with weights [<tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677f9d68>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fc358>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fc908>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fceb8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677fa4a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677faa90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677800b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677806a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867780c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677832b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867783898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867783e80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677874a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867787a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677880b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677886a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867788c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677942b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867794898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867794e80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677994a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867799a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677a50b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677a56a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677a5c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677ad2b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677ad898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677ade80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677b24a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677b2a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677bb0b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677bb6a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677bbc88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677452b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867745898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867745e80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86774b4a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86774ba90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677500b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677506a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867750c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86775a2b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86775a898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff86775ae80>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677614a8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867761a90>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677680b8>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677686a0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867768c88>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff8677712b0>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867771898>, <tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7ff867771e80>]"
     ]
    }
   ],
   "source": [
    "# save the best performing model to file\n",
    "model.save('/home/danish/Codes/Auto_Keras_Dataset_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(automl.show_models(), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = automl.predict(X_test)\n",
    "print(\"Accuracy score:\", sklearn.metrics.accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, stratify=y)\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "  \n",
    "# clf = AutoSklearnClassifier(time_left_for_this_task=600,\n",
    "#                             max_models_on_disc=5,\n",
    "#                             memory_limit = 102400,\n",
    "#                             resampling_strategy=skf,\n",
    "#                             ensemble_size = 3,\n",
    "#                             metric = average_precision,\n",
    "#                             scoring_functions=[roc_auc, average_precision, accuracy, f1, precision, recall, log_loss])    \n",
    "\n",
    "# clf.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = '/home/danish/Datasets/ICS/binaryAllNaturalPlusNormalVsAttacks'\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and concatenate its data to the combined DataFrame\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path).dropna()\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "data = combined_df # .sample(frac=0.5, random_state=42)\n",
    "data.shape\n",
    "# Optionally, you can save the combined DataFrame to a new CSV file\n",
    "# combined_df.to_csv('/path/to/combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of bad or constant columns \n",
    "columns_to_drop = ['R3-PA:Z', 'R1-PA:Z', 'R2-PA:Z', 'R4-PA:Z', 'snort_log1', 'snort_log2', 'control_panel_log2', 'control_panel_log1']\n",
    "\n",
    "# Remove specified columns\n",
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)\n",
    "# data = pd.read_csv('/home/danish/Downloads/Datasets/ICS/binaryAllNaturalPlusNormalVsAttacks/data1.csv')\n",
    "data.replace([np.inf, -np.inf], 1e15, inplace=True)\n",
    "# data\n",
    "\n",
    "# Extract the numerical columns for normalization\n",
    "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data[numerical_cols]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = pd.DataFrame(data.drop('marker', axis=1)).corr()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "sns.heatmap(corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "et.fit(data.drop('marker', axis=1), data['marker'].tolist())\n",
    "importances = et.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(75,50))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(data.drop('marker', axis=1).shape[1]), importances[indices],\n",
    "        color=\"lightsalmon\", align=\"center\")\n",
    "plt.xticks(range(data.drop('marker', axis=1).shape[1]), data.drop('marker', axis=1).columns[indices], rotation=90)\n",
    "plt.xlim([-1, data.drop('marker', axis=1).shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' contains your DataFrame with features and labels\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "labels = data['marker']  # Labels\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "reduced_df = pd.DataFrame(components, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "# Plot PCA visualization with colored points based on labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_df['Component 1'], reduced_df['Component 2'], c=range(0,78377), cmap='viridis')\n",
    "plt.title('PCA Visualization of Feature Vectors with Colorful Labels')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Label (Marker)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' contains your data\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "label = data['marker']  # Labels\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting 3D PCA visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(components[:, 0], components[:, 1], components[:, 2], c=range(0,78377), cmap='viridis')\n",
    "ax.set_title('PCA Visualization (3D)')\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# sfm = SelectFromModel(rf, threshold=0.00025)\n",
    "sfm = SelectFromModel(et)\n",
    "sfm.fit(data.drop('marker', axis=1), data['marker'].tolist())\n",
    "# X_important_train = sfm.transform(X_train)\n",
    "# X_important_test = sfm.transform(X_test)\n",
    "\n",
    "feature_vector_1 = sfm.transform(data.drop('marker', axis=1))\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "# rf.fit(X_important_train, y_train)\n",
    "# y_pred = rf.predict(X_important_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "print(feature_vector_1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sfm.get_feature_names_out()\n",
    "feature_vector_1 =pd.DataFrame(feature_vector_1)\n",
    "feature_vector_1.columns = cols\n",
    "feature_vector_1\n",
    "\n",
    "\n",
    "\n",
    "corr = pd.DataFrame(feature_vector_1).corr()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "sns.heatmap(corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "et.fit(feature_vector_1, data['marker'].tolist())\n",
    "importances = et.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(75,50))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(feature_vector_1.shape[1]), importances[indices],\n",
    "        color=\"lightsalmon\", align=\"center\")\n",
    "plt.xticks(range(feature_vector_1.shape[1]), feature_vector_1.columns[indices], rotation=90)\n",
    "plt.xlim([-1, feature_vector_1.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = feature_vector_1.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Normalize the numerical columns\n",
    "feature_vector_1[numerical_cols] = scaler.fit_transform(feature_vector_1[numerical_cols])\n",
    "feature_vector_1\n",
    "# # Display the normalized DataFrame\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' contains your DataFrame with features and labels\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "labels = data['marker']  # Labels\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "reduced_df = pd.DataFrame(components, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "# Plot PCA visualization with colored points based on labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_df['Component 1'], reduced_df['Component 2'], c=range(0,78377), cmap='viridis')\n",
    "plt.title('PCA Visualization of Feature Vectors with Colorful Labels')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Label (Marker)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' contains your data\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "label = data['marker']  # Labels\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting 3D PCA visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(components[:, 0], components[:, 1], components[:, 2], c=range(0,78377), cmap='viridis')\n",
    "ax.set_title('PCA Visualization (3D)')\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming the target variable is in a column named 'target'\n",
    "# X = data.drop('marker', axis=1)\n",
    "X = feature_vector_1\n",
    "y = data['marker']\n",
    "# y.value_counts()\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import autokeras as ak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train AutoKeras CNN model\n",
    "clf = ak.StructuredDataClassifier(max_trials=10, overwrite=True)\n",
    "clf.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate AutoKeras model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"AutoKeras CNN Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Train traditional Machine Learning model (Random Forest)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(\"Random Forest Model Accuracy: \", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get predictions\n",
    "y_pred = automl.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Plot ROC curve and calculate AUC\n",
    "y_proba = automl.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
