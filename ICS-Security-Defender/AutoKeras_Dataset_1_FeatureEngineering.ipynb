{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/anaconda3/envs/AutoML/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-06-26 11:27:45.806085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 11:27:46.386254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/danish/anaconda3/envs/AutoML/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.layers.experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, roc_curve, auc\n",
      "File \u001b[0;32m~/anaconda3/envs/AutoML/lib/python3.9/site-packages/autokeras/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The AutoKeras Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertBlock\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CategoricalToNumerical\n",
      "File \u001b[0;32m~/anaconda3/envs/AutoML/lib/python3.9/site-packages/autokeras/auto_model.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m blocks\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph \u001b[38;5;28;01mas\u001b[39;00m graph_module\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[0;32m~/anaconda3/envs/AutoML/lib/python3.9/site-packages/autokeras/blocks/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertBlock\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvBlock\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseBlock\n",
      "File \u001b[0;32m~/anaconda3/envs/AutoML/lib/python3.9/site-packages/autokeras/blocks/basic.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_layers\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduction\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m block \u001b[38;5;28;01mas\u001b[39;00m block_module\n",
      "File \u001b[0;32m~/anaconda3/envs/AutoML/lib/python3.9/site-packages/autokeras/keras_layers.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautokeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_utils\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import autokeras as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from pycm import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = '/home/danish/Datasets/ICS/binaryAllNaturalPlusNormalVsAttacks'\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and concatenate its data to the combined DataFrame\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path).dropna()\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "data = combined_df # .sample(frac=0.5, random_state=42)\n",
    "data.shape\n",
    "# Optionally, you can save the combined DataFrame to a new CSV file\n",
    "# combined_df.to_csv('/path/to/combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of bad or constant columns \n",
    "columns_to_drop = ['R3-PA:Z', 'R1-PA:Z', 'R2-PA:Z', 'R4-PA:Z', 'snort_log1', 'snort_log2', 'control_panel_log2', 'control_panel_log1']\n",
    "\n",
    "# Remove specified columns\n",
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)\n",
    "# data = pd.read_csv('/home/danish/Downloads/Datasets/ICS/binaryAllNaturalPlusNormalVsAttacks/data1.csv')\n",
    "data.replace([np.inf, -np.inf], 1e15, inplace=True)\n",
    "# data\n",
    "\n",
    "# Extract the numerical columns for normalization\n",
    "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data[numerical_cols]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = pd.DataFrame(data.drop('marker', axis=1)).corr()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "sns.heatmap(corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "et.fit(data.drop('marker', axis=1), data['marker'].tolist())\n",
    "importances = et.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(75,50))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(data.drop('marker', axis=1).shape[1]), importances[indices],\n",
    "        color=\"lightsalmon\", align=\"center\")\n",
    "plt.xticks(range(data.drop('marker', axis=1).shape[1]), data.drop('marker', axis=1).columns[indices], rotation=90)\n",
    "plt.xlim([-1, data.drop('marker', axis=1).shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' contains your DataFrame with features and labels\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "labels = data['marker']  # Labels\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "reduced_df = pd.DataFrame(components, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "# Plot PCA visualization with colored points based on labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_df['Component 1'], reduced_df['Component 2'], c=range(0,78377), cmap='viridis')\n",
    "plt.title('PCA Visualization of Feature Vectors with Colorful Labels')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Label (Marker)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' contains your data\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "label = data['marker']  # Labels\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting 3D PCA visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(components[:, 0], components[:, 1], components[:, 2], c=range(0,78377), cmap='viridis')\n",
    "ax.set_title('PCA Visualization (3D)')\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# sfm = SelectFromModel(rf, threshold=0.00025)\n",
    "sfm = SelectFromModel(et)\n",
    "sfm.fit(data.drop('marker', axis=1), data['marker'].tolist())\n",
    "# X_important_train = sfm.transform(X_train)\n",
    "# X_important_test = sfm.transform(X_test)\n",
    "\n",
    "feature_vector_1 = sfm.transform(data.drop('marker', axis=1))\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "# rf.fit(X_important_train, y_train)\n",
    "# y_pred = rf.predict(X_important_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "print(feature_vector_1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sfm.get_feature_names_out()\n",
    "feature_vector_1 =pd.DataFrame(feature_vector_1)\n",
    "feature_vector_1.columns = cols\n",
    "feature_vector_1\n",
    "\n",
    "\n",
    "\n",
    "corr = pd.DataFrame(feature_vector_1).corr()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "sns.heatmap(corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators = 100, class_weight='balanced', random_state=42)\n",
    "et.fit(feature_vector_1, data['marker'].tolist())\n",
    "importances = et.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(75,50))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(feature_vector_1.shape[1]), importances[indices],\n",
    "        color=\"lightsalmon\", align=\"center\")\n",
    "plt.xticks(range(feature_vector_1.shape[1]), feature_vector_1.columns[indices], rotation=90)\n",
    "plt.xlim([-1, feature_vector_1.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = feature_vector_1.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Normalize the numerical columns\n",
    "feature_vector_1[numerical_cols] = scaler.fit_transform(feature_vector_1[numerical_cols])\n",
    "feature_vector_1\n",
    "# # Display the normalized DataFrame\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' contains your DataFrame with features and labels\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "labels = data['marker']  # Labels\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "reduced_df = pd.DataFrame(components, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "# Plot PCA visualization with colored points based on labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_df['Component 1'], reduced_df['Component 2'], c=range(0,78377), cmap='viridis')\n",
    "plt.title('PCA Visualization of Feature Vectors with Colorful Labels')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Label (Marker)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' contains your data\n",
    "features1 = data.drop('marker', axis=1)  # Features\n",
    "label = data['marker']  # Labels\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to 2 dimensions for visualization\n",
    "components = pca.fit_transform(features1)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting 3D PCA visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(components[:, 0], components[:, 1], components[:, 2], c=range(0,78377), cmap='viridis')\n",
    "ax.set_title('PCA Visualization (3D)')\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming the target variable is in a column named 'target'\n",
    "# X = data.drop('marker', axis=1)\n",
    "X = feature_vector_1\n",
    "y = data['marker']\n",
    "# y.value_counts()\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import autokeras as ak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train AutoKeras CNN model\n",
    "clf = ak.StructuredDataClassifier(max_trials=10, overwrite=True)\n",
    "clf.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate AutoKeras model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"AutoKeras CNN Model Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Train traditional Machine Learning model (Random Forest)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(\"Random Forest Model Accuracy: \", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get predictions\n",
    "y_pred = automl.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Plot ROC curve and calculate AUC\n",
    "y_proba = automl.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
